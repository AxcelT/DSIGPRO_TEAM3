import whisper
import librosa
import numpy as np
import os
import matplotlib.pyplot as plt
import soundfile as sf

# Hyperparameters
energy_threshold = 0.06

# Logging Helper
def log_message(message):
    """Helper function to log messages."""
    print(f"[INFO] {message}")

# Step 1: Preprocessing
def preprocess_audio(file_path, target_sr=16000):
    """Convert audio to mono and resample to the target sampling rate."""
    log_message(f"Loading audio file: {file_path}")
    audio, sr = librosa.load(file_path, sr=None)  # Load audio with original sampling rate
    log_message(f"Original sampling rate: {sr}")
    if sr != target_sr:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
        log_message(f"Audio resampled to {target_sr} Hz.")
    return audio, target_sr

# Step 2: Short Speech Detection
def detect_short_segments(audio, sr, max_duration=2, min_energy=energy_threshold, min_gap=0.5, min_silence_duration=0.3):
    """
    Detect short speech segments based on energy thresholds and duration.
    Modified to prevent splitting longer speeches into smaller segments due to brief silences.
    """
    log_message("Detecting short speech segments...")
    frame_length = int(0.025 * sr)  # 25ms frames
    hop_length = int(0.010 * sr)   # 10ms hops
    energy = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length).flatten()
    times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)

    short_segments = []
    segment_start = None
    silence_duration = 0
    for i, e in enumerate(energy):
        if e > min_energy:
            if segment_start is None:
                segment_start = times[i]
            silence_duration = 0  # Reset silence duration
        else:
            if segment_start is not None:
                # Accumulate silence duration
                if i > 0:
                    silence_duration += times[i] - times[i - 1]
                else:
                    silence_duration += times[i]
                # Check if silence duration exceeds min_silence_duration
                if silence_duration >= min_silence_duration:
                    segment_end = times[i]
                    segment_duration = segment_end - segment_start
                    if segment_duration <= max_duration:
                        if not short_segments or (segment_start - short_segments[-1][1]) > min_gap:
                            short_segments.append((segment_start, segment_end))
                            log_message(f"Short segment detected: {segment_start:.2f}s - {segment_end:.2f}s")
                    segment_start = None
                    silence_duration = 0

    # Handle case where the last segment goes till the end of the audio
    if segment_start is not None:
        segment_end = times[-1]
        segment_duration = segment_end - segment_start
        if segment_duration <= max_duration:
            if not short_segments or (segment_start - short_segments[-1][1]) > min_gap:
                short_segments.append((segment_start, segment_end))
                log_message(f"Short segment detected: {segment_start:.2f}s - {segment_end:.2f}s")

    log_message(f"Total short segments detected: {len(short_segments)}")
    return short_segments

# Step 3: Save Short Segments for Subtitle Generation
def save_segments(audio, sr, segments, output_dir, base_filename, min_segment_duration=1.0):
    """Save detected short speech segments as individual audio files, ensuring minimum 1-second duration."""
    log_message(f"Saving detected segments to '{output_dir}'...")
    os.makedirs(output_dir, exist_ok=True)
    saved_files = []
    for i, (start, end) in enumerate(segments):
        start_sample = int(start * sr)
        end_sample = int(end * sr)

        # Adjust the end_sample to ensure a minimum segment duration
        if end - start < min_segment_duration:
            end_sample = start_sample + int(min_segment_duration * sr)
            # Ensure we don't exceed the audio length
            if end_sample > len(audio):
                end_sample = len(audio)

        segment_audio = audio[start_sample:end_sample]
        adjusted_end = end_sample / sr  # Adjusted end time for logging
        output_file = os.path.join(output_dir, f"{base_filename}_segment_{i+1}.wav")
        sf.write(output_file, segment_audio, sr)
        saved_files.append((output_file, start, adjusted_end))  # Store file path and times

        log_message(f"Segment saved: {output_file} ({start:.2f}s - {adjusted_end:.2f}s)")
    return saved_files

# Step 4: Visualization
def plot_energy(audio, sr, energy, times, segments, output_dir, base_filename):
    """Plot the energy curve with detected segments."""
    log_message("Plotting energy curve with detected segments...")

    plt.figure(figsize=(26, 8))  # Increase the width to make the plot wider
    plt.plot(times, energy, label="RMS Energy")
    plt.axhline(y=energy_threshold, color="red", linestyle="--", label="Energy Threshold")
    
    # Highlight detected segments
    for start, end in segments:
        plt.axvspan(start, end, color="green", alpha=0.3)

    # Customize x-axis ticks for 1-second intervals
    max_time = times[-1]
    plt.xticks(np.arange(0, max_time + 1, 1))  # Set x-axis ticks every 1 second
    
    plt.xlabel("Time (s)")
    plt.ylabel("Energy")
    plt.title(f"Energy Curve for {base_filename}")
    plt.legend()
    plt.grid(True)

    # Save the plot
    plot_path = os.path.join(output_dir, f"{base_filename}_energy_curve.png")
    plt.savefig(plot_path)
    log_message(f"Energy curve saved: {plot_path}")
    plt.close()

# Step 5: Generate Subtitles
# Step 5: Generate Subtitles
def generate_subtitles(segment_files_with_times, output_dir, base_filename):
    """
    Generate subtitle file using Whisper.
    :param segment_files_with_times: List of tuples (file_path, start_time, end_time).
    :param output_dir: Directory to save the subtitles.
    :param base_filename: Base name for the subtitle file.
    """
    log_message("Generating subtitles...")
    model = whisper.load_model("base")  # Load Whisper model
    srt_file_path = os.path.join(output_dir, f"{base_filename}.srt")

    def format_timestamp(seconds):
        """Format seconds as HH:MM:SS,ms for subtitles."""
        hours, remainder = divmod(seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        milliseconds = int((seconds - int(seconds)) * 1000)
        return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}"

    with open(srt_file_path, "w") as srt_file:
        for i, (segment_file, start_time, end_time) in enumerate(segment_files_with_times):
            log_message(f"Processing segment: {segment_file}")
            
            # Transcribe the audio segment
            result = model.transcribe(segment_file)
            text = result["text"]
            
            # Format timestamps
            start_timestamp = format_timestamp(start_time)
            end_timestamp = format_timestamp(end_time)
            
            # Write subtitle entry
            srt_file.write(f"{i + 1}\n")
            srt_file.write(f"{start_timestamp} --> {end_timestamp}\n")
            srt_file.write(f"{text.strip()}\n\n")
            
            log_message(f"Finished processing segment: {segment_file}")

    log_message(f"Subtitles saved to: {srt_file_path}")
    return srt_file_path


    log_message(f"Subtitles saved to: {srt_file_path}")
    return srt_file_path




# Main Script
if __name__ == "__main__":
    # Input audio file
    audio_file = r"audio\Frasier1.wav"
    audio_dir = r"audio"

    
    if not os.path.exists(audio_file):
        raise FileNotFoundError(f"Audio file not found: {audio_file}") # Verify the file exists
    base_filename = os.path.splitext(os.path.basename(audio_file))[0]

    # Output directories
    output_base_dir = r"D:\School\DSIGPRO_Project\outputs"
    segments_dir = os.path.join(output_base_dir, "segments")
    plots_dir = os.path.join(output_base_dir, "plots")

    # Ensure output directories exist
    os.makedirs(output_base_dir, exist_ok=True)
    os.makedirs(segments_dir, exist_ok=True)
    os.makedirs(plots_dir, exist_ok=True)

 # Process each .wav file in the audio directory
    for audio_file in os.listdir(audio_dir):
        if audio_file.endswith(".wav"):
            audio_path = os.path.join(audio_dir, audio_file)
            base_filename = os.path.splitext(audio_file)[0]
            
            log_message(f"Processing file: {audio_path}")

            # Step 1: Preprocess audio
            audio, sr = preprocess_audio(audio_path)

            # Log basic audio properties
            log_message(f"Audio Duration: {len(audio) / sr:.2f}s")
            log_message(f"Audio Sample Rate: {sr}")

            # Step 2: Detect short segments
            short_segments = detect_short_segments(audio, sr, max_duration=1.5, min_energy=0.02, min_gap=0.1)

            # Step 3: Save short segments
            if short_segments:
                saved_files_with_times = save_segments(audio, sr, short_segments, segments_dir, base_filename)
                log_message(f"Saved {len(saved_files_with_times)} short segments.")

                # Step 4: Visualize energy curve
                frame_length = int(0.025 * sr)
                hop_length = int(0.010 * sr)
                energy = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length).flatten()
                times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)
                plot_energy(audio, sr, energy, times, short_segments, plots_dir, base_filename)
                log_message(f"Plot generated: {plots_dir}")

                # Step 5: Generate subtitles with segment times
                srt_file_path = generate_subtitles(saved_files_with_times, segments_dir, base_filename)
                log_message(f"Subtitles generated: {srt_file_path}")
            else:
                log_message("No short speech segments detected.")

    log_message("Processing completed for all files!")